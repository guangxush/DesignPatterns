## Scalability Rules - 50 Principles for Scaling Web Sites

## Reduce the Equation

### Rule 1 Don’t Overengineer the Solution

- What: Guard against complex solutions during design.
- When to use: Can be used for any project and should be used for all large or complex systems or projects.
- How to use: Resist the urge to overengineer solutions by testing ease of understanding with fellow engineers.
- Why: Complex solutions are costly to implement and have excessive long-term costs.
- Key takeaways: Systems that are overly complex limit your ability to scale. Simple systems are more easily and cost effectively maintained and scaled.

### Rule 2 Design Scale into the Solution (D-I-D Process)

- What: An approach to provide JIT (Just In Time) Scalability. When to use: On all projects; this approach is the most cost effective (resources and time) to ensure scalability.
- How to use:  Design for 20x capacity.  Implement for 3x capacity.  Deploy for ~1.5x capacity.
- Why: D-I-D provides a cost effective, JIT method of scaling your product.
- Key takeaways: Teams can save a lot of money and time by thinking of how to scale solutions early, implementing (coding) them a month or so before they are needed, and implementing them days before the customer rush or demand.


### Rule 3 Simplify the Solution 3 Times Over

- What: Used when designing complex systems, this rule simplifies the scope, design, and implementation.
- When to use: When designing complex systems or products where resources (engineering or computational) are limited.
- How to use:  Simplify scope using the Pareto Principle.  Simplify design by thinking about cost effectiveness and scalability.  Simplify implementation by leveraging the experience of others.
- Why: Focusing just on “not being complex” doesn’t address the issues created in requirements or story and epoch development or the actual implementation.
- Key takeaways: Simplification needs to happen during every aspect of product development.

### Rule 4 Reduce DNS Lookups

- What: Reduce the number of DNS lookups from a user perspective.
- When to use: On all Web pages where performance matters.
- How to use: Minimize the number of DNS lookups required to download pages, but balance this with the browser’s limitation for simultaneous connections.
- Why: DNS lookups take a great deal of time, and large numbers of them can amount to a large portion of your user experience.
- Key takeaways: Reduction of objects, tasks, computation, and so on is a great way of speeding up page load time, but division of labor must be considered as well.


### Rule 5 Reduce Objects Where Possible

- What: Reduce the number of objects on a page where possible. 
- When to use: On all web pages where performance matters. 
- How to use: Reduce or combine objects but balance this with maximizing simultaneous connections. Test changes to ensure performance improvements. Why: The number of objects impacts page download times.
- Key takeaways: The balance between objects and methods that serve them is a science that requires constant measurement and adjustment; it’s a balance between customer usability, usefulness, and performance.


### Rule 6 Use Homogenous Networks


- What: Don’t mix the vendor networking gear.
- When to use: When designing or expanding your network. 
- How to use:Do not mix different vendors’ networking gear (switches and routers).Buy best of breed for other networking gear (firewalls, load balancers, and so on).
- Why: Intermittent interoperability and availability issues simply aren’t worth the potential cost savings.
- Key takeaways: Heterogeneous networking gear tends to cause availability and scalability problems. Choose a single provider.

## Distribute Your Work

At the heart of the AKF Scale Cube are three simple axes, each with an associated rule for scalability.The cube is a great way to represent the path from minimal scale (lower-left front of the cube) to near infinite scalability (upper-right back corner of the cube). Sometimes, it’s easier to see these three axes without the confined space of the cube. 

![](../../image/AFK.png)


![](../../image/threerulesofAFK.png)

### Rule 7 Design to Clone Things (X Axis)


- What: Typically called horizontal scale, this is the duplication of services or databases to spread transaction load.
- When to use: Databases with a very high read to write ratio (5:1 or greater—the higher the better). Any system where transaction growth exceeds data growth. 
- How to use: Simply clone services and implement a load balancer. For databases, ensure the accessing code understands the difference between a read and a write.
- Why: Allows for fast scale of transactions at the cost of duplicated data and functionality.
- Key takeaways: X axis splits are fast to implement and can allow for transaction, but not data scalability.


### Rule 8 Design to Split Different Things (Y Axis)

- What: Sometimes referred to as scale through services or resources, this rule focuses on scaling data sets, transactions, and engineering teams.
- When to use: Very large data sets where relations between data are not necessary. Large, complex systems where scaling engineering resources requires specialization.
- How to use: Split up actions by using verbs or resources by using nouns or use a mix. Split both the services and the data along the lines defined by the verb/noun approach.
- Why: Allows for efficient scaling of not only transactions, but very large data sets associated with those transactions.
- Key takeaways: Y axis or data/service-oriented splits, allow for efficient scaling of transactions, large data sets, and can help with fault isolation.


### Rule 9 Design to Split Similar Things (Z Axis)

- What: This is very often a split by some unique aspect of the customer such as customer ID, name, geography, and so on.
- When to use: Very large, similar data sets such as large and rap- idly growing customer bases.
- How to use: Identify something you know about the customer, such as customer ID, last name, geography, or device and split or partition both data and services based on that attribute.
- Why: Rapid customer growth exceeds other forms of data growth or you have the need to perform fault isolation between certain customer groups as you scale.
- Key takeaways: Z axis splits are effective at helping you to scale customer bases but can also be applied to other very large data sets that can’t be pulled apart using the Y axis methodology.


We maintain that three simple rules can help you scale nearly everything.There are undoubtedly more ways to scale systems and platforms, but armed with these three rules, few if any scale related problems will stand in your way:

- Scale by cloning: Cloning or duplicating data and services allows you to scale transactions easily.
- Scale by splitting different things: Use nouns or verbs to identify data and services to separate. If done properly, both transactions and data sets can be scaled efficiently.
- Scale by splitting similar things: Typically these are customer data sets. Set customers up into unique and separated shards or swimlanes (see Chapter 9,“Design for Fault Tolerance and Graceful Failure,” for swimlane definition) to enable transaction and data scaling.

## Design to Scale Out Horizontally

### Rule 10: Design Your Solution to Scale Out—Not Just Up

- What: Scaling out is the duplication of services or databases to spread transaction load and is the alternative to buying larger hardware, known as scaling up.
- When to use: Any system, service, or database expected to grow rapidly.
- How to use: Use the AKF Scale Cube to determine the correct split for your environment. Usually the horizontal split (cloning) is the easiest.
- Why: Allows for fast scale of transactions at the cost of duplicat- ed data and functionality.
- Key takeaways: Plan for success and design your systems to scale out. Don’t get caught in the trap of expecting to scale up only to find out that you’ve run out of faster and larger systems to purchase.


### Rule 11: Use Commodity Systems (Goldfish Not Thoroughbreds)

- What: Use small, inexpensive systems where possible.
- When to use: Use this approach in your production environment when going through hyper growth.
- How to use: Stay away from very large systems in your produc- tion environment.
- Why: Allows for fast, cost-effective growth.
- Key takeaways: Build your systems to be capable of relying on commodity hardware and don’t get caught in the trap of using high-margin, high-end servers.


### Rule 12：Scale Out Your Data Centers

- What: Design your systems to have three or more live data cen- ters to reduce overall cost, increase availability, and implement disaster recovery.
- When to use: Any rapidly growing business that is considering adding a disaster recovery (cold site) data center.
- How to use: Split up your data to spread across data centers and spread transaction load across those data centers in a “mul- tiple live” configuration. Use spare capacity for peak periods of the year.
- Why: The cost of data center failure can be disastrous to your business. Design to have three or more as the cost is often less than having two data centers. Make use of idle capacity for peak periods rather than slowing down your transactions.
- Key takeaways: When implementing disaster recovery, lower your cost of disaster recovery by designing your systems to leverage three or more live data centers. Use the spare capacity for spiky demand when necessary.

Split of data center replication

![](../../image/datareplication.png)


Two data center configuration, “hot and cold” site

![](../../image/twodatacenter.png)

Three data center configuration, three hot sites

![](../../image/threedatacenter.png)

### Rule 13: Design to Leverage the Cloud

- What: This is the purposeful utilization of cloud technologies to scale on demand.
- When to use: When demand is temporary, spiky, and inconsistent and when response time is not a core issue in the product.
- How to use: Make use of third-party cloud environments for temporary demands, such as large batch jobs or QA environments during testing cycles. Design your application to service some requests from a third-party cloud when demand exceeds a certain peak level.
- Why: Provisioning of hardware in a cloud environment takes a few minutes as compared to days or weeks for physical servers in your own collocation facility. When utilized temporarily this is also very cost effective.
- Key takeaways: Design to leverage virtualization and the cloud to meet unexpected spiky demand.

### Rule 14: Use Databases Appropriately

- What: Use relational databases when you need ACID properties to maintain relationships between your data. For other data storage needs consider more appropriate tools.
- When to use: When you are introducing new data or data structures into the architecture of a system.
- How to use: Consider the data volume, amount of storage, response time requirements, relationships, and other factors to choose the most appropriate storage tool.
- Why: An RDBMS provides great transactional integrity but is more difficult to scale, costs more, and has lower availability than many other storage options.
- Key takeaways: Use the right storage tool for your data. Don’t get lured into sticking everything in a relational database just because you are comfortable accessing data in a database.

Cost and limits to scale versus relationships:

![](../../image/EmbeddedRelationships.png)

Flexibility versus relationships:

![](../../image/Flexibility.png)

Solution decision cube:

![](../../image/decisioncube.png)

### Rule 15: Firewalls, Firewalls Everywhere!

- What: Use firewalls only when they significantly reduce risk and recognize that they cause issues with scalability and availability.
- When to use: Always.
- How to use: Employ firewalls for critical PII, PCI compliance, and so on. Don’t use them for low-value static content.
- Why: Firewalls can lower availability and cause unnecessary scal- ability chokepoints.
- Key takeaways: While firewalls are useful, they are often over- used and represent both an availability and scalability concern if not designed and implemented properly.


### Rule 16: Actively Use Log Files

- What: Use your application’s log files to diagnose and prevent problems.
- When to use: Put a process in place that monitors log files and forces people to take action on issues identified.
- How to use: Use any number of monitoring tools from custom scripts to Splunk to watch your application logs for errors. Export these and assign resources for identifying and solving the issue.
- Why: The log files are excellent sources of information about how your application is performing for your users; don’t throw this resource away without using it.
- Key takeaways: Make good use of your log files, and you will have fewer production issues with your system.

![](../../image/Logaggregation.png)


## Don’t Duplicate Your Work

### Rule 17: Don’t Check Your Work

- What: Avoid checking things you just did or reading things you just wrote within your products.
- When to use: Always (see rule conflict in the following explanation).
- How to use: Never read what you just wrote for the purpose of validation. Store data in a local or distributed cache if it is required for operations in the near future.
- Why: The cost of validating your work is high relative to the unlikely cost of failure. Such activities run counter to cost- effective scaling.
- Key takeaways: Never justify reading something you just wrote for the purposes of validating the data. Read and act upon errors associated with the write activity instead. Avoid other types of reads of recently written data by storing that data locally.

Here is a bulleted checklist of questions you can answer and steps you can take to eliminate reading what you just wrote and blocking the user transaction to do so:
1. Regulatory/legal requirement
2. Competitive differentiation
3. Asynchronous completion


### Rule 18: Stop Redirecting Traffic

- What: Avoid redirects when possible; use the right method when they are necessary.
- When to use: Use redirects as little as possible.
- How to use: If you must have them, consider server configurations instead of HTML or other code-based solutions.
- Why: Redirects in general delay the user, consume computation resources, and are prone to errors.
- Key takeaways: Use redirects correctly and only when necessary.

### Rule 19: Relax Temporal Constraints

- What: Alleviate temporal constraints in your system whenever possible.
- When to use: Any time you are considering adding a constraint that an item or object maintains a certain state between a user’s actions.
- How to use: Relax constraints in the business rules.
- Why: The difficulty in scaling systems with temporal constraints is significant because of the ACID properties of most RDBMSs.
- Key takeaways: Carefully consider the need for constraints such as items being available from the time a user views them until the user purchases them. Some possible edge cases where users are disappointed are much easier to compensate for than not being able to scale.


These requirements are expressed in the acronym CAP:
- Consistency—The client perceives that a set of operations has occurred all at once.
- Availability—Every operation must terminate in an intended response.
- Partition tolerance—Operations will complete, even if individual components are unavailable.

What has been derived as a solution to this problem is called BASE, an acronym for architectures that solve CAP and stands for Basically Available, Soft State, and Eventually Consistent. By relaxing the ACID properties of consistency we have greater flexibility in how we scale. A BASE architecture allows for the databases to become consistent, eventually.


Summary:

We offered three rules in this chapter that deal with not duplicating your work. Start by not double checking yourself. You employ expensive databases and hardware to ensure your systems properly record transactions and events. Don’t expect them not to work.We all have the need for redirection at times, but excessive use of this tool causes all types of problems from user experience to search engine indexing. Finally, consider the business requirements that you place on your system.Temporal constraints of items and objects make it difficult and expensive to scale. Carefully consider the real costs and benefits of these decisions.


## Use Caching Aggressively

### Rule 20: Leverage Content Delivery Networks

- What: Use CDNs (content delivery networks) to offload traffic from your site.
- When to use: Ensure it is cost justified and then choose which content is most suitable.
- How to use: Most CDNs leverage DNS (Domain Name Services or Domain Name Servers) to serve content on your site’s behalf.
- Why: CDNs help offload traffic spikes and are often economical ways to scale parts of a site’s traffic.
- Key takeaways: CDNs are a fast and simple way to offset spiki- ness of traffic as well as traffic growth in general. Make sure you perform a cost-benefit analysis and monitor the CDN usage.


![](../../image/cdnexample.png)

### Rule 21: Use Expires Headers

- What: Use Expires headers to reduce requests and improve the scalability and performance of your system.
- When to use: All object types need to be considered.
- How to use: Headers can be set on Web servers or through application code.
- Why: The reduction of object requests increases the page performance for the user and decreases the number of requests your system must handle per user.
- Key takeaways: For each object type (IMAGE, HTML, CSS, PHP, and so on) consider how long the object can be cached for and implement the appropriate header for that timeframe.


### Rule 22: Cache Ajax Calls

- What: Use appropriate HTTP response headers to ensure cacheability of Ajax calls.
- When to use: Every Ajax call but those absolutely requiring real time data that are likely to have been recently updated.
- How to use: Modify Last-Modified, Cache-Control, and Expires headers appropriately.
- Why: Decrease user perceived response time, increase user satisfaction, and increase the scalability of your platform or solution.
- Key takeaways: Leverage Ajax and cache Ajax calls as much as possible to increase user satisfaction and increase scalability.

### Rule 23: Leverage Page Caches

- What: Deploy page caches in front of your Web services. When to use: Always.
- How to use: Choose a caching system and deploy.
- Why: Decrease load on Web servers by caching and delivering previously generated dynamic requests and quickly answering calls for static objects.
- Key Takeaways: Page caches are a great way to offload dynamic requests and to scale cost effectively.

![](../../image/proxycache.png)

### Rule 24: Utilize Application Caches

- What: Alleviate temporal constraints in your system whenever possible.
- When to use: Any time you are considering adding a constraint that an item or object maintains a certain state between a user’s actions.
- How to use: Relax constraints in the business rules.
- Why: The difficulty in scaling systems with temporal constraints is significant because of the ACID properties (see definition in Chapter 2, “Distribute Your Work”) of most RDBMSs (Relational Database Management Systems).
- Key takeaways: Carefully consider the need for constraints such as items being available from the time a user views it until they purchase. Some possible edge cases where users are disap- pointed are much easier to compensate for than not being able to scale.

### Rule 25: Make Use of Object Caches

- What: Implement object caches to help your system scale. 
- When to use: Any time you have repetitive queries or computations.
- How to use: Select any one of the many open source or vendor supported solutions and implement the calls in your application code.
- Why: A fairly straightforward object cache implementation can save a lot of computational resources on application servers or database servers.
- Key takeaways: Consider implementing an object cache any- where computations are performed repeatedly, but primarily this is done between the database and application tiers.


### Rule 26: Put Object Caches on Their Own “Tier”

- What: Use a separate tier in your architecture for object caches. 
- When to use: Any time you have implemented object caches. 
- How to use: Move object caches onto their own servers.
- Why: The benefits of a separate tier are better utilization of mem- ory and CPU resources and having the ability to scale the object cache independently of other tiers.
- Key takeaways: When implementing an object cache it is sim- plest to put the service on an existing tier such as the applica- tion servers. Consider implementing or moving the object cache to its own tier for better performance and scalability.


The advantage of separating these tiers is that you can size the servers appropriately in terms of how much memory and CPU are required, and you can scale the number of servers in this pool independently of other pools. Sizing the server correctly can save quite a bit of money since object caches typically require a lot of memory—most all store the objects and keys in memory—but require relatively low computational processing power.You can also add servers as necessary and have all the additional capacity utilized by the object cache rather than split- ting it with an application or Web service.

![](../../image/objectcache.png)


### Learn from Your Mistakes


#### Rule 27: Learn Aggressively

- What: Take every opportunity to learn.
- When to use: Be constantly learning from your mistakes as well as successes.
- How to use: Watch your customers or use A/B testing to deter- mine what works. Use postmortems to learn from incidents and problems in production.
- Why: Doing something without measuring the results or having an incident without learning from it are wasted opportunities that your competitors are taking advantage of.
- Key takeaways: Be constantly and aggressively learning. The companies that learn best, fastest, and most often are the ones that grow the fastest and are the most scalable.


#### Rule 28: Don’t Rely on QA to Find Mistakes

- What: Use QA to lower the cost of delivered products, increase engineering throughput, identify quality trends, and decrease defects—not to increase quality.
- When to use: Whenever you can get greater throughput by hiring someone focused on testing rather than writing code. Use QA to learn from past mistakes—always.
- How to use: Hire a QA person anytime you get greater than one engineer’s worth of output with the hiring of a single QA person.
- Why: Reduce cost, increase delivery volume/velocity, decrease the number of repeated defects.
- Key takeaways: QA doesn’t increase the quality of your system, as you can’t test quality into a system. If used properly, it can increase your productivity while decreasing cost and most importantly it can keep you from increasing defect rates faster than your rate of organization growth during periods of rapid hiring.



#### Rule 29: Failing to Design for Rollback Is Designing for Failure

- What: Always have the ability to roll back code.
- When to use: Ensure all releases have the ability to roll back, practice it in a staging or QA environment, and use it in production when necessary to resolve customer incidents.
- How to use: Clean up your code and follow a few simple procedures to ensure you can roll back your code.
- Why: If you haven’t experienced the pain of not being able to roll back, you likely will at some point if you keep playing with the “fix-forward” fire.
- Key takeaways: Don’t accept that the application is too complex or that you release code too often as excuses that you can’t roll back. No sane pilot would take off in an airplane without the ability to land, and no sane engineer would roll code that they could not pull back off in an emergency.

By going through the application to clean up any outstanding issues and then adhering to some simple rules every team should be able to roll back.

- Database changes must only be additive
- DDL and DML scripted and tested
- Restricted SQL queries in the application
- Semantic changes of data
- Wire On/Wire Off

#### Rule 30: Discuss and Learn from Failures

- What: Leverage every failure to learn and teach important les- sons.
- When to use: Always.
- How to use: Employ a postmortem process and hypothesize failures in low failure environments.
- Why: We learn best from our mistakes—not our successes.
- Key takeaways: Never let a good failure go to waste. Learn from every one and identify the technology, people, and process issues that need to be corrected.

Todd LaPorte, who developed the theory of High Reliability Organizations, believes that even in the case of an absence of accidents from which an organization can learn, there are organizational strategies to achieve higher reliability.

For any major issue that we experience, we believe an organization should attack that issue with a postmortem process that addresses the problem in three distinct but easily described phases:

- Phase 1 Timeline—Focus on generating a timeline of the events leading up to the issue or crisis. 
- Phase 2 Issue Identification—The process facilitator walks through the timeline and works with the team to identify issues.
- Phase 3 State Actions—Each item should have at least one action associated with it.

Their solution, as modified to fit our needs, is described as follows:

- Preoccupation with failure—This practice is all about monitoring our product and our systems and reporting errors in a timely fashion. 
- Reluctance to simplify interpretations—Take nothing for granted and seek input from diverse sources. 
- Sensitivity to operations—Look at detail data at the minute level. Include the usage of real time data and make ongoing assessments and continual updates of this data.
- Commitment to resilience—Build excess capability by rotating positions and training your people in new skills.
- Deference to expertise—During crisis events, shift the leadership role to the person possessing the greatest expertise to deal with the problem.


In ten years you will be the same person you are today except for the people you meet and the books you read.


### Database Rules

ACID Properties of Databases

| Rule | Description  |
| :--------:   | :----- |
|**A**tomicity | All of the operations in the transaction will complete, or none will. |
|**C**onsistency |The database will be in a consistent state when the transaction begins and ends. |
|**I**solation | The transaction will behave as if it is the only operation being performed upon the database. | 
|**D**urability |Upon completion of the transaction, the operation will not be reversed. | 

#### Rule 31: Be Aware of Costly Relationships

- What: Be aware of relationships in the data model.
- When to use: When designing the data model, adding tables/columns, or writing queries consider how the relationships between entities will affect performance and scalability in the long run.
- How to use: Think about database splits and possible future data needs as you design the data model.
- Why: The cost of fixing a broken data model after it has been implemented is likely 100x as much as fixing it during the design phase.
- Key takeaways: Think ahead and plan the data model carefully. Consider normalized forms, how you will likely split the database in the future, and possible data needs of the application.

Normal Forms:

Here are the most common normal forms used in databases. Each higher normal form implies that it must satisfy lower forms. Generally a database is said to be in normal form if it adheres to third normal form.

- First normal form—Originally, as defined by Codd,1 the table should represent a relation and have no repeating groups. While “relation” is fairly well defined by Codd, the meaning of “repeating groups” is a source of debate. Controversy exists over whether tables are allowed to exist within tables and whether null fields are allowed. The most important concept is the ability to create a key.
- Second normal form—Nonkey fields cannot be described by only one of the keys in a composite key.
- Third normal form—All nonkey fields must be described by the key.
- Boyce-Codd normal form—Every determinant is a candi- date key.
- Fourth normal form—A record type should not contain two or more multivalued facts.
- Fifth normal form—Every nontrivial join dependency in the table is implied by the candidate keys.
- Sixth normal form—No nontrivial join dependencies exist. 

An easy mnemonic for the first three normal forms is “1 — The Key, 2 — The Whole Key, and 3 — Nothing But the Key.”

When SQL queries perform poorly because of the require- ments to join tables there are several alternatives.The first is to tune the query. If this doesn’t help another alternative is to cre- ate a view, materialized view, summary table, and so on that can preprocess the joins.Another alternative is to not join in the query but rather pull the data sets into the application and join in memory in the application.While this is more complex it removes the processing of the join off the database, which is often the most difficult to scale and puts it in the application server tier, which is easier to scale out with more commodity hardware.A final alternative is to push back on the business requirements. Often our business partners will come up with different solutions when it is explained that the way they have requested the report requires a 10% increase in hardware while the removal of a single column may make the report trivial in complexity and nearly as equivalent in business value.


#### Rule 32: Use the Right Type of Database Lock

- What: Be cognizant of the use of explicit locks and monitor implicit locks.
- When to use: Anytime you employ relational databases for your solution.
- How to use: Monitor explicit locks in code reviews. Monitor data- bases for implicit locks and adjust explicitly as necessary to mod- erate throughput. Choose a database and storage engine that allows flexibility in types and granularity of locking.
- Why: Maximize concurrency and throughput in databases within your environment.
- Key takeaways: Understand the types of locks and manage their usage to maximize database throughput and concurrency. Change lock types to get better utilization of databases and look to split schemas or distribute databases as you grow. When choosing databases, ensure you choose one that allows multiple lock types and granularity to maximize concurrency.


| Type of Lock | Description  |
| :--------:   | :----- |
|Implicit | Implicit locks are those generated by the database on behalf of a user to perform certain transactions. These are typically generated when necessary for certain DML (Data Manipulation Language) tasks. |
|Explicit | These are locks defined by the user of a data- base during the course of his interaction with entities within the database. |
|Row | Row level locking locks a row in a table of a database that is being updated, read, or created. | 
|Page | Page level locking locks the entire page that contains a row or group of rows being updated. | 
|Extent | Typically, these are locks on groups of pages. They are common when database space is being added. | 
|Table | Locks an entire table (an entity within a database). | 
|Database |Locks the entirety of entities and relationships within a database. | 

While locking is absolutely critical to the operations of a database to facilitate both isolation and consistency, it is obvious- ly costly.Typically databases allow reads to occur simultaneously on data, while blocking all reads during the course of a write (an update or insertion) on an element undergoing an operation. Reads then can occur very fast, and many of them can happen at one time while typically a write happens in isolation.The finer the granularity of the write operation, such as in the case of a single row, the more of these can happen within the data- base or even within a table at a time. Increasing the granularity of the object being written or updated, such as updating multi- ple rows at a time, may require an escalation of the type of lock necessary.


#### Rule 33: Pass on Using Multiphase Commits

- What: Do not use a multiphase commit protocol to store or process transactions.
- When to use: Always pass or alternatively “never use” multi- phase commits.
- How to use: Don’t use it; split your data storage and processing systems with Y or Z axis splits.
- Why: Multiphase commits are blocking protocols that do not per- mit other transactions from occurring until it is complete.
- Key takeaways: Do not use multiphase commit protocols as a simple way to extend the life of your monolithic database. It will likely cause it to scale even less and result in an even earlier demise of your system.

Multiphase commit protocols, which include the popular two-phase commit (2PC) and three-phase commit (3PC), are specialized consensus protocols.The purpose of these protocols is
to coordinate processes that participate in a distributed atomic
transaction to determine whether to commit or abort (roll back) the transaction. Because of these algorithms’ capability to
handle systemwide failures of the network or processes, they are often looked to as solutions for distributed data storage or processing.


The basic algorithm of 2PC consists of two phases.The first phase, voting phase, is where the master storage or coordinator makes a “commit request” to all the cohorts or other storage devices.All the cohorts process the transaction up to the point of committing and then acknowledge that they can commit or vote “yes.”Thus begins the second phase or completion phase, where the master sends a commit signal to all cohorts who begin the commit of the data. If any cohorts should fail during the com- mit then a rollback is sent to all cohorts and the transaction is abandoned. An example of this protocol is shown in Figure.

![](../../image/2pcexample.png)


#### Rule 34: Try Not to Use “Select For Update”

- What: Minimize the use of the FOR UPDATE clause in a SELECT statement when declaring cursors.
- When to use: Always.
- How to use: Review cursor development and question every SELECT FOR UPDATE usage.
- Why: Use of FOR UPDATE causes locks on rows and may slow down transactions.
- Key takeaways: Cursors are powerful constructs that when prop- erly used can actually make programming faster and easier while speeding up transactions. But FOR UPDATE cursors may cause long held locks and slow transactions. Refer to your database documentation for whether you need to use the FOR READ ONLY clause to minimize locks.


#### Rule 35: Don’t Select Everything


- What: Don’t use Select * in queries.
- When to use: Always use this rule (or put another way, never select everything).
- How to use: Always declare what columns of data you are select- ing or inserting in a query.
- Why: Selecting everything in a query is prone to break things when the table structure changes and it transfers unneeded data.
- Key takeaways: Don’t use wildcards when selecting or inserting data.


### Design for Fault Tolerance and Graceful Failure

In our business, availability and scalability go hand in hand. A product that isn’t highly available really doesn’t need to scale and a site that can’t scale won’t be highly available when the demand comes. As such, you really can’t work on one without thinking about the other.This chapter offers rules that help ensure sites can both scale AND be resilient to and tolerant of failures while still delivering value to the customer.


#### Rule 36: Design Using Fault Isolative “Swimlanes”

- What: Implement fault isolation or swimlanes in your designs. 
- When to use: Whenever you are beginning to split up databases to scale.
- How to use: Split up databases and services along the Y or Z axis and disallow synchronous communication or access between services.
- Why: Increase availability and scalability and reduce incident identification and resolution as well as time to market and cost.
- Key takeaways: Fault isolation consists of eliminating synchro- nous calls between fault isolation domains, limiting asynchronous calls and handling synchronous call failure, and eliminating the sharing of services and data between swimlanes.

Types of Splits:

| Split Name | Description  |
| :--------:   | :----- |
|Pod | Pods are self-contained sets of functionality containing app servers, persistent storage (such as a database or other persistent and shared file system), or both. Pods are most often splits along the Z axis, as in a split of customers into separate pods. “Pod” is sometimes used interchangeably with the term “swimlane.” It has also been used interchangeably with the term “pool” when referring to Web or application services. |
|Cluster | Clusters are sometimes used for Web and application servers in the same fashion as a “pool” identified next. In these cases a cluster refers to an X axis scale of sim- ilar functionality or purpose configured such that all nodes or participants are “active.” Often a cluster will share some sort of distributed state above and beyond that of a pool, but this state can cause scalability bottle- necks under high transaction volumes. Clusters might also refer to active/passive configuration where one (or more) devices sit “passive” and become “active” on the failure of a peer device. |
|Pool | Pools are servers that group similar functionality or poten- tially separate groups of customers. The term typically refers to front end servers, but some companies refer to database service pools for certain characteristics. Pools are typically X axis replicated (cloned) servers that are demarcated by function (Y axis) or customer (Z axis). | 
|Shard | Shards are horizontal partitions of databases or search engines. Horizontal partitioning means the separation of data across database tables, database instances, or physical database servers. Shards typically occur along the Z axis of scale (for instance splitting up customers among shards), but some companies refer to functional (Y axis) splits as shards as well. | 
|Swimlane | A swimlane is a term used to identify a fault isolation domain. Synchronous calls are never allowed across swimlane boundaries. Put another way, a swimlane is defined around a set of synchronous calls. The failure of a component within one swimlane does not affect com- ponents in other swimlanes. As such, no component is shared across swimlanes. | 


Fault Isolation Benefits:

| Area | Benefit  |
| :--------:   | :----- |
|Availability | Availability is increased as a failure within one failure domain does not impact other services (Y axis) or other customers (Z axis) depending on how the swimlane is architected. |
|Incident detection | Incidents are detected faster as fewer components or services need to be investigated during an event. Isolation helps identify what exactly is failing. |
|Scalability | Horizontal scale is achieved when fault isolated services can grow independently of each other. | 
|Cost | Development cost is reduced through higher engineer throughput achieved from focus and specialization. | 
|Time to market | As throughput increases, time to market for functions decreases. | 


Fault Isolation Principles:

| Principle | Benefit  |
| :--------:   | :----- |
|Share nothing | Swimlanes should not share services. Some sharing of network gear such as border routers and load balancers is acceptable. If necessary, storage area networks can be shared. Databases and servers should never be shared. |
|No synchronous calls between swimlanes | Synchronous calls are never allowed across swimlane boundaries. Put another way, a swimlane is the smallest unit across which no synchronous calls happen. |
|Limit asynchronous calls between swimlanes | Asynchronous calls should be limited across swimlanes. They are permitted, but the more calls that are made, the greater the chance of failure propagation. | 
|Use timeouts and Wire-On/Wire-Off with asynchronous calls | Asynchronous calls should be implemented with timeouts and the ability to turn off the call when necessary due to failures in other services. | 

#### Rule 37: Never Trust Single Points of Failure

- What: Never implement and always eliminate single points of failure.
- When to use: During architecture reviews and new designs. 
- How to use: Identify single instances on architectural diagrams. Strive for active/active configurations.
- Why: Maximize availability through multiple instances.
- Key takeaways: Strive for active/active rather than active/pas- sive solutions. Use load balancers to balance traffic across instances of a service. Use control services with active/passive instances for patterns that require singletons.



SPOFs can be anywhere in the system from a single Web server or single network device but most often the SPOF in a system is the database.The reason for this is that the database is often the most difficult to scale across multiple nodes and there- fore gets left as a singleton. In Figure 9.1, even though there are redundant login, search, and checkout servers the database is a SPOF.What makes it worse is that all the service pools are reliant on that single database.While any SPOF is bad, the bigger problem with a database as a SPOF is if the database slows down or crashes, all services pools with synchronous calls to that data- base will also experience an incident.

![](../../image/spof.png)

The solution to most SPOFs is simply requisitioning another
piece of hardware and running two or more of every service by cloning that service as described in our X axis of scale. Unfortunately, this isn’t always so easy. Let’s retrace our steps to the programming singleton pattern.While not all singleton class- es will prevent you from running a service on multiple servers, some implementations absolutely will prevent you from this without dire consequences. As a simplified example, if we have
a class in our code that handles the subtraction of funds from a user’s account this might be implemented as a singleton to prevent unpleasant things from happening to a user’s balance such as it going negative. If we place this code on two separate servers without additional controls or semaphores it is possible that two simultaneous transactions attempt to debit a users account, which could lead to erroneous or undesired conditions. In this case we need to either fix the code to handle this condi- tion or rely on an external control to prevent this condition. While the most desirable solution is to fix the code so that the service can be implemented on many different hosts, often we need an expeditious fix to remove a SPOF.As the last focus of this rule, we’ll discuss a few of these quick fixes next.

The first and simplest solution is to use an active-passive con- figuration.The service would run actively on one server and passively (not taking traffic) on a second server.This hot/cold configuration is often used with databases as a first step in removing a SPOF.The next alternative would be to use another component in the system to control the access to data. If the SPOF was a service, then the database can be used to control access to data through the use of locks. If the SPOF is the data- base, a master-slave configuration can be set up, and the applica- tion can control access to the data with writes/updates going to the master and reads/selects going to the slave.A last configura- tion that can be used to fix a SPOF is a load balancer. If the service on a Web or application server was a SPOF and could not be eliminated in code the load balancer can often be used to fix a user’s request to only one server in the pool.This is done through session cookies, which are set on the user’s browser and allow the load balancer to redirect that user’s requests to the same Web or application server each time resulting in a consistent state.


#### Rule 38: Avoid Putting Systems in Series

- What: Reduce the number of components that are connected in series.
- When to use: Anytime you are considering adding components. 
- How to use: Remove unnecessary components or add multiple versions of them to minimize the impact.
- Why: Components in series have a multiplicative effect of failure.
- Key takeaways: Avoid adding components to your system that are connected in series. When necessary to do so add multiple versions of that component so that if one fails others are avail- able to take its place.


This brings us back to your real world architecture.Almost always there are going to be requirements to have components in series.When you take into consideration the load balancer, the Web and application tier, the database, the storage system,
and so on there are many components required to keep your system running. Certainly adding components in parallel, even when tiers are connected in series, helps reduce the risk of total system failure caused by a component failure. Multiple Web servers spread the traffic load and prevent a system failure if only one Web server fails. On the Web and application tiers most people readily acknowledge this concept.Where most people overlook this problem is in the database and network layers. If Web and application servers connected in parallel all are con- nected in series to a single database we can have a single compo- nent result in catastrophic failure.This is why it is important to pay attention to the rules in Chapter 2 about splitting your data- base and in Chapter 3,“Design to Scale Out Horizontally,” about scaling horizontally.


#### Rule 39: Ensure You Can Wire On and Off Functions

- What: Create a framework to disable and enable features of your product.
- When to use: Risky, very high use, or shared services that might otherwise cause site failures when slow to respond or unavailable.
- How to use: Develop shared libraries to allow automatic or on-demand enabling and disabling of services. See Table 9.5 for recommendations.
- Why: Graceful failure (or handling failures) of transactions can keep you in business while you recover from the incident and problem that caused it.
- Key takeaways: Implement Wire On/Wire Off Frameworks whenever the cost of implementation is less than the risk and associated cost of failure. Work to develop shared libraries that can be reused to lower the cost of future implementation.


Wire On/Wire Off Approaches: 

| Approach | Description  | Pro | Con |
| :--------: | :----- | :----- | :----- |
| Automatic markdown based on timeouts | Useful for synchronous  calls for internal and external services. Calls are not made upon markdown at all, and service is considered “offline.” | Fastest way to mark down a service that might bring several other services down when slow or unavailable | Sensitive to “false failures” or incorrect identification of a failing service. When coupled with auto markup, may cause a “pinging” effect of the service. Every service needs to make its own decision. |                                                                                                                                       down when slow or unavailable.| |
| Stand in service | Replace a service with an auto responder with a dummy response that indicates service unavailable or a cached response of “likely good data.” | Easy to implement, at least on the service side. May allow user determination of failure.|Each calling service needs to understand the “failure” response. May be slower to mark down and may require user inter- vention to mark up. |
| Synchronous markdown command |User intervention sends a command to services to stop using the failed or slow service. | Allows user determination of failed service.|Slower than automatic mark- down. Also, if services have TCP ports full due to slow or failed service, the com- mand may not work as desired. Requires user intervention to mark up. |
| Config file markdown |Change configuration file variable to indicate the "wire off" of a service | Doesn't rely on request/reply communication as in a synchronous command. | Likely requires restart of a server to implement |
| File markdown | Presence of a file (or absence of a file) indicates service up or down (can be used or not). |Doesn’t rely on request/reply communication as in a synchronous command. Might not require server restart. | May slow down processes “polling” for files. |
| Database markdown  | Use of a variable (column) per service in a database to enable or disable features. | Can be done on restart or per request. Easy to communicate to all servers by changing one location. | Requires one or more “control tables” that need to be highly avail- able and may need to be repli- cated so as not to cross swimlane boundaries. If done on a trans- action basis can be costly. |
| Runtime variable |  Read at startup as an argument to the program for daemon-like server processes.| Similar to config file. | Similar to config file. |


### Avoid or Distribute State


Decision flowchart for implementing state in a Web application:

![](../../image/webapplication.png)

#### Rule 40: Strive for Statelessness

- What: Design and implement stateless systems.
- When to use: During design of new systems and redesign of existing systems.
- How to use: Choose stateless implementations whenever possible. If stateful implementations are warranted for business reasons, refer to Rules 41 and 42.
- Why: The implementation of state limits scalability and increases cost.
- Key takeaways: Always push back on the need for state in any system. Use business metrics and multivariate (or A/B) testing to determine whether state in an application truly results in the expected user behavior and business value.


#### Rule 41: Maintain Sessions in the Browser When Possible

- What: Try to avoid session data completely, but when needed, consider putting the data in users’ browsers.
- When to use: Anytime that you need session data for the best user experience.
- How to use: Use cookies to store session data on the users’ browsers.
- Why: Keeping session data on the users’ browsers allows the user request to be served by any Web server in the pool and takes the storage requirement away from your system.
- Key takeaways: Using cookies to store session data is a com- mon approach and has advantages in terms of ease of scale but also has some drawbacks. One of the most critical cons is that unsecured cookies can easily be captured and used to log into people’s accounts.


#### Rule 42: Make Use of a Distributed Cache for States


- What: Use a distributed cache when storing session data in your system.
- When to use: Anytime you need to store session data and cannot do so in users’ browsers.
- How to use: Watch for some common mistakes such as a ses- sion management system that requires affinity of a user to a Web server.
- Why: Careful consideration of how to store session data can help
ensure your system will continue to scale.
- Key takeaways: Many Web servers or languages offer simple server-based session management, but these are often fraught with problems such as user affiliation with specific servers. Implementing a distributed cache allows you to store session data in your system and continue to scale.

Distributed Session/State Cache Don’ts. Here are three approaches to avoid in implementing a cache to manage session or state:
- Don’t implement systems that require affinity to a server to function properly.
- Don’t use state or session replication to create duplicates of data on different systems.
- Don’t locate the cache on the system doing the work (this doesn’t mean you shouldn’t have a local application cache—just that session information is best handled in its own tier of servers).


Distributed Session/State Cache Considerations. Here are three common implementations for distributed caches and some notes on their benefits and drawbacks:
- Database-only implementations are the most costly overall, but allow all data to be persisted and handle conflicts between updates and reads very well in a distributed envi- ronment.
- Nonpersistent object caches are fast and comparatively inexpensive, but do not allow data to be recovered upon failure and aren’t good for implementations with long peri- ods between accesses by users.
- Hybrid solutions with databases providing persistency and caches providing cost-effective scale are great when persistency is required and low relative cost is preferred.

### Asynchronous Communication and Message Buses

#### Rule 43: Communicate Asynchronously As Much As Possible

- What: Use asynchronous instead of synchronous communication as often as possible.
- When to use: Consider for all calls between services and tiers. 
- How to use: Use language specific calls to ensure the requests are made and not waited on.
- Why: Synchronous calls stop the entire program’s execution wait- ing for a response, which ties all the services and tiers together resulting in cascading failures.
- Key takeaways: Use asynchronous communication techniques to ensure that each service and tier is as independent as possible. This allows the system to scale much farther than if all compo- nents are closely coupled together.


The way to determine which calls are asynchronous candidates is to analyze each call based on the criteria such as the following:

- External API/third party
- Long running processes
- Error prone/changed frequently methods
- Temporal constraint

#### Rule 44: Ensure Your Message Bus Can Scale

- What: Message buses can fail from demand like any other physical or logical system. They need to be scaled.
- When to use: Anytime a message bus is part of your architecture.
- How to use: Employ the Y and Z AKF Axes of Scale. 
- Why: To ensure your bus scales to demand.
- Key takeaways: Treat message buses like any other critical component of your system. Scale them ahead of demand using either the Y or Z axes of scale.


![](../../image/afkcube.png)

| Attribute Split | Pro  | Con  |
| :--------: | :----- | :----- |
|Temporal | Monitoring for failures to meet response times is easy—just look for the oldest message against an absolute standard.|Not all messages are created equal. Some may be small and fast but not necessary for critical function completion. |
|Service |Only connects systems that need to communicate with each other.|Reduction in flexibility with various nodes connected in affinity fashion.|
|Quality of service |Costs to scale and make any bus highly available can scale in accordance with the importance of the message.|Likely to still need a way to scale buses with a lot of traffic for either highly important or unimportant messages.|
|Resource|Similar types of data (rather than the services) share a bus. Simple logical implementation.|May require some services to listen for infrequent messages on a bus.|


#### Rule 45: Avoid Overcrowding Your Message Bus


- What: Limit bus traffic to items of higher value than the cost to handle them.
- When to use: On any message bus.
- How to use: Value and cost justify message traffic. Eliminate low value, high cost traffic. Sample low value/low cost and high value/high cost traffic to reduce the cost.
- Why: Message traffic isn’t “free” and presents costly demand on your system.
- Key takeaways: Don’t publish everything. Sample traffic to ensure alignment between cost and value.


Cost/value relationship of data and corresponding message bus action:

![](../../image/costvalue.png)



### Miscellaneous Rules


#### Rule 46: Be Wary of Scaling Through Third Parties

- What: Scale your own system; don’t rely on vendor solutions to achieve scalability.
- When to use: Whenever considering whether to use a new fea- ture or product from a vendor.
- How to use: Rely on the rules of this book for understanding how to scale and use vendor provided products and services in the most simplistic manner possible.
- Why: Three reasons for following this rule: Own your destiny, keep your architecture simple, and reduce your total cost of ownership.
- Key takeaways: Do not rely on vendor products, services, or fea- tures to scale your system. Keep your architecture simple, keep your destiny in your own hands, and keep your costs in control. All three of these can be violated by using a vendor’s proprietary scaling solution.


#### Rule 47: Purge, Archive, and Cost-Justify Storage

- What: Match storage cost to data value, including removing data of value lower than the costs to store it.
- When to use: Apply to data and its underlying storage infrastruc- ture during design discussions and throughout the lifecycle of the data in question.
- How to use: Apply recency, frequency, and monetization analysis to determine the value of the data. Match storage costs to data value.
- Why: Not all data is created equal (that is, of the same value), and in fact it often changes in value over time. Why then should we have a single storage solution with equivalent cost for that data?
- Key takeaways: It is important to understand and calculate the value of your data and to match storage costs to that value. Don’t pay for data that doesn’t have a stakeholder return.


![](../../image/rfmstorage.png)


The X axis of our repurposed cube addresses the frequency
of access.The axis moves from data that is “never” (or very infrequently) accessed to that which is accessed constantly or always.TheY axis of the cube identifies recency of access and has low values of never to high values of the data being accessed right now.The Z axis of the cube deals with monetization, from values of no value to very high value. Using the cube as a method of analysis, one could plot potential solutions along the multiple dimensions of the cube. Data in the lower left and front portion of the cube has no value and was never accessed, meaning that we should purge this data if regulatory conditions allow us to do so. Why would we incur a cost for data that won’t return value to
our business? The upper right and back portion of our three- dimensional cube identifies the most valuable pieces of business data.We strive to store these on the solutions with the highest reliability and fastest access times such that the transactions that use them can happen quickly for our clients. Ideally we would cache this data somewhere as well as having it on a stable storage solution, but the underlying storage solution might be the fastest solid state disks that current technology supports.These disks might be striped and mirrored for access speed and high availability.

The product of our RFM analysis might yield a score for the value of the data overall. Maybe it’s as simple as a product or maybe you’ll add some magic of your own that actually applies some dollar value to the resulting score. If we employed this dollar value score to a value curve that matched the resulting RFM value to the cost of a solution to support it we might end up with a diagram similar to that of Figure

![](../../image/rfmvalue.png)

#### Rule 48: Remove Business Intelligence from Transaction Processing

- What: Separate business systems from product systems and product intelligence from database systems.
- When to use: Anytime you are considering internal company needs and data transfer within, to, or from your product.
- How to use: Remove stored procedures from the database and put them in your application logic. Do not make synchronous calls between corporate and product systems.
- Why: Putting application logic in databases is costly and repre- sents scale challenges. Tying corporate systems and product sys- tems together is also costly and represents similar scale chal- lenges as well as availability concerns.
- Key takeaways: Databases and internal corporate systems can be costly to scale due to license and unique system characteris- tics. As such, we want them dedicated to their specific tasks. In the case of databases, we want them focused on transactions rather than product intelligence. In the case of back office sys- tems (business intelligence), we do not want our product tied to their capabilities to scale. Use asynchronous transfer of data for business systems.

We often tell our clients to steer clear of stored procedures with- in relational databases. One of their first questions is typically “Why do you hate stored procedures so much?”The truth is that we don’t dislike stored procedures. In fact, we’ve used them with great effect on many occasions.The problem is that stored procedures are often overused within solutions, and this overuse sometimes causes scalability bottlenecks in systems that would otherwise scale efficiently and almost always results in a very high cost of scale. Given the emphasis on databases, why didn’t we put this rule in the chapter on databases? The answer is that the drivers of our concerns over stored procedures are really driven by the need to separate business intelligence and product intelligence from transaction processing. In general, this concept
can be further abstracted to “Keep like transactions together (or alternatively separate unlike transactions) for the highest possible availability and scalability and best possible cost.”Those are a lot of words for a principle, so let’s first return to our concern over stored procedures and databases as an illustration as to why this separation should occur.


#### Rule 49: Design Your Application to Be Monitored

- What: Think about how you will need to monitor your application as you are designing it.
- When to use: Anytime you add or change modules of your code base.
- How to use: Build hooks into your system to record transaction times.
- Why: Having insight into how your application is performing will help answer many questions when there is a problem.
- Key takeaways: Adopt as an architectural principle that your application must be monitored. Additionally, look at your overall monitoring strategy to make sure you are first answering the question of “Is there a problem?” and then the “Where” and “What.”

![](../../image/monitoring.png)


#### Rule 50: Be Competent


- What: Be competent, or buy competency in/for each component of your architecture.
- When to use: For any Internet service or commerce solution.
- How to use: For each component of your infrastructure,
identify the team responsible and level of competency with that component.
- Why: To a customer, every problem is your problem. You can’t blame suppliers or providers. You provide a service—not software.
- Key takeaways: Don’t confuse competence with build versus buy or core versus context decisions. You can buy solutions and still be competent in their deployment and maintenance. In fact, your customers demand that you do so.


Your customers expect you to deliver a service to them.
To that end, the development of unique software to create that service is a means to an end.You are, at the end of the day, in the service business. Make no mistake about that. It is a mindset requirement that when not met has resulted in the deterioration and even death of companies. Friendster’s focus on the “F-graph,” the complex solution that calculated relationships within the social network, was at least one of the reasons Facebook won the private social network race.At the heart of this focus was an attitude held within many software shops—a focus that the challenging problem of the F-graph needed to be
solved.This focus led to a number of outages within the site, or very slow response times as systems ground to a halt while attempting to compute relationships in near real time. Contrast this with a focus on a service, where availability and response time are more important than any particular feature. Software is just a means for providing that service.


### Rule Review and Prioritization

#### A Risk–Benefit Model for Evaluating Scalability Projects and Initiatives


It is within this context of concern over availability and scalability that we represent our concept of risk.The risk of an incident caused by the inability to scale manifests itself as a threat to our quality of service or availability. One method of calculat- ing risk is to look at the probability that a problem will happen multiplied by its impact should it happen (or move from risk to issue). Figure shows this method of risk decomposition.

![](../../image/risk.png)

The terminal nodes (nodes without children) of the decomposition graph of Figure are leaves within our risk tree: Probability of a Problem, % Customers Impacted, % Functionality Impacted, Downtime, % Data Loss, and Response time Impact. Looking at these leaves, we can see that many of our rules map to these leaves. For instance, Rule 1 is really about decreasing the probability of a problem happening by ensuring that the system is easily understood and therefore less likely to be problematic. It may also decrease downtime as the solution is likely to be easier to troubleshoot and resolve. Using a bit
of subjective analysis, we may decide that this rule has a small (or low) benefit to impact and a medium change to the probability of a problem happening.The result of this may
be that the rule has an overall medium impact to risk
(low + medium = ~medium).


Our prioritization equation then becomes risk reduction minus cost equals priority or (R – C = P).Table 13.1 shows how we computed the nine permutations of risk and cost and the resulting priority.The method we chose was simple.The benefit for any equation where risk and cost were equivalent was set to medium and the priority set to the midrange of 3.Where risk reduction was two levels higher than cost, the benefit was set to Very High, and the Priority set to 1.Where risk reduction was two levels lower than cost (Low Risk, High Cost) the bene- fit was set to Very Low, and priority set to 5. Differences of one were either Low (Risk Reduction Low and Cost Medium) with a priority score of 4or High (Risk Reduction Medium and Cost Low) with a priority score of 2.The projects with the lowest
priority score have the highest benefit and are the first things we will do.

Risk Reduction, Cost, and Benefit Calculation:


| Risk Reduction | Cost  | Resulting Benefit/Priority  |
| :--------: | :----- | :----- |
|High|High|Medium 3|
|High|Medium|High 2|
|High|Low|Very High 1|
|Medium|High|Low 4|
|Medium|Medium|Medium 3|
|Medium|Low|High 2|
|Low|High|Very Low 5|
|Low|Medium|Low 4|
|Low|Low|Medium 3|


#### A Benefit/Priority Ranking of the Scalability Rules

***Very High – 1***

- Rule 19 Relax Temporal Constraints
- Rule 25 Make Use of Object Caches
- Rule 29 Failing to Design for Rollback Is Designing for Failure
- Rule 30 Discuss and Learn from Failures
- Rule 32 Use the Right Type of Database Lock
- Rule 35 Don’t Select Everything
- Rule 46 Be Wary of Scaling Through Third Parties
- Rule 50 Be Competent


***High – 2***

- Rule 1 Don’t Overengineer the Solution
- Rule 7 Design to Clone Things (X Axis)
- Rule 10 Design Your Solution to Scale Out—Not Just Up
- Rule 11 Use Commodity Systems (Goldfish Not Thoroughbreds)
- Rule 14 Use Databases Appropriately
- Rule 15 Firewalls, Firewalls, Everywhere!
- Rule 22 Cache Ajax Calls
- Rule 26 Put Object Caches on Their Own “Tier”
- Rule 27 Learn Aggressively
- Rule 28 Don’t Rely on QA to Find Mistakes
- Rule 33 Pass on Using Multiphase Commits
- Rule 34 Try Not to Use “Select For Update”
- Rule 37 Never Trust Single Points of Failure
- Rule 41 Maintain Sessions in the Browser When Possible
- Rule 42 Make Use of a Distributed Cache for States
- Rule 43 Communicate Asynchronously As Much As Possible
- Rule 44 Ensure Your Message Bus Can Scale
- Rule 45 Avoid Overcrowding Your Message Bus
- Rule 48 Remove Business Intelligence from Transaction Processing
- Rule 49 Design Your Application to Be Monitored

***Medium – 3***

- Rule 2 Design Scale into the Solution (D-I-D Process)
- Rule 3 Simplify the Solution 3 Times Over
- Rule 4 Reduce DNS Lookups
- Rule 5 Reduce Objects Where Possible
- Rule 6 Use Homogenous Networks
- Rule 8 Design to Split Different Things (Y Axis)
- Rule 9 Design to Split Similar Things (Z Axis)
- Rule 12 Scale Out Your Data Centers
- Rule 16 Actively Use Log Files
- Rule 18 Stop Redirecting Traffic
- Rule 20 Leverage CDNs
- Rule 21 Use Expires Headers
- Rule 23 Leverage Page Caches
- Rule 24 Utilize Application Caches
- Rule 31 Be Aware of Costly Relationships
- Rule 36 Design Using Fault Isolative “Swimlanes”
- Rule 38 Avoid Putting Systems in Series
- Rule 40 Strive for Statelessness
- Rule 47 Purge, Archive, and Cost-Justify Storage


***Low – 4***

- Rule 13 Design to Leverage the Cloud
- Rule 17 Don’t Check Your Work
- Rule 39 Ensure You Can Wire On and Off Functions

***Very Low – 5***

- N/A
